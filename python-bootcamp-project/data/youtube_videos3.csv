source,title,description,view_count,thumbnail_url,publish_date,length,author,video_url,page_content
FRogt98OF80,Build An MCP Server In 5 Prompts // Vibe Coding That Doesn't Suck,"The Prompts: https://gist.github.com/mattpocock/0aae0ed9b604750f07dee0ea75d8b03d

Follow Matt on Twitter

https://twitter.com/mattpocockuk

Join the Discord:

https://mattpocock.com/discord",30751,https://i.ytimg.com/vi/FRogt98OF80/maxresdefault.jpg,2025-04-08 08:30:07,762,Matt Pocock,https://www.youtube.com/watch?v=FRogt98OF80,"What's up, folks? I'm going to try something different today. I'm going to build an MCP server in five prompts. I'm going to make these prompts available to you so you can follow along. And we're hopefully going to learn a little bit of something about MCP servers, but also how to do AI assisted development because yeah, you've got vibe coding on one side and I like to think of there as AI assisted development on the other, the serious stuff. And it turns out that planning and documentation and doing the boring stuff is really, really important for this kind of sensible side of dev. We're starting off with an empty directory in cursor here. And to start off, I'm just going to paste in this prompt. And while that's running, I'm just going to show you the structure of the prompt that we have here. There are three main sections. We have a problem, which is essentially a description of the problem that this prompt is designed to solve. We want to set up a new MCP server written in Typescript. We're starting from an empty directory. We need to set up the basic file system, install the dependencies, set up the project structure. The supporting information is all the information that the LLM needs to carry out the task. In this case, I've added sections on the tools that we want to use for for instance, pmppm and then also on the proposed file structure that I would like to use here. So, stuff about the important files MDC file, package.json, tsconfig, source main.ts, getit ignore, etc. And finally, the steps to complete are literally the step-by-step things that the uh LLM should do in order to complete the task. You don't need to manually write this all out. You know, you can use autocomplete or you can prompt actually to get this structure out for you. But I found these three top level headings to be really useful in just focusing my writing and my prompting. Okay, let's see what it put together. We've got a tsconfig file which is basically ripped verbatim from the prompt. We've got a pnpm lock file which is nice. So it it knows that we're supposed to use pnpm. We've got a package.json file with a bunch of stuff that we're supposed to be in here. In the prompt, our specified type module bin uh build and dev and all the dependencies and stuff. We've got our git ignore with disc and node modules. Very nice. And we've got a source directory with a bunch of stuff that we specified in the prompt. This is ripped pretty much verbatim from model context protocols SDK readme. And for now it's using the standard IO server transport. We can see that the LLM got stuck in a couple of places. It's it kind of tried to install the wrong version of model context protocol SDK and then it was corrected by this big beautiful error message. Very very nice. And actually that was the only place. So it followed the step-by-step instructions really nicely. Let's head to prompt number two. Our MCP server currently doesn't do anything. Let's hook it up to GitHub so it can respond to issues and pull requests. We're going to use OctoKit for this, which is GitHub's SDK. It's going to need to know about tsx and in the supporting information. I'm also telling it to always declare tool descriptions cuz when you call server.tool, you can either pass in a name or pass in a name and a description. And a description is always nicer because it means that the MCP client knows what this tool actually does. Then finally in the steps to complete I'm giving it loads of functions that I want it to write. Now in the app let's open up a new chat and paste this in. So it's done a few things already. It's added to git ignore. It's created a demo file for me. Very nice. And it's also updated the dev script to use the N file using N file from TSX. I'm pretty sure that in the pre-training data it didn't know how to do this. It doesn't know TSX that well. And so I made sure I added that in the supporting information and now it doesn't. Now it's asking to install OctoKit, which is very nice. I really like it when it uses pnpm to install Octo cuz it means that it gets the latest version automatically instead of just guessing the version that was in its pre-training data. Now it's creating this massive GitHub functions file. It's done a really good job here. Like I've barely included any information about Octo and it's just managing to cleanly bof slam it out. I assume this is because the GitHub REST API has been around for a long time. So it's probably in the training data somewhere. It's also updated the important files.mmdc to add this GitHub file to it and get ignore and m very nice. And finally, it's now added all of these different tools here. Get pull request, create issue comment. If I scan through these, we can see all of them. Create pull request comment and all of these like extra zod parameters to make sure that the MCP client can call it correctly. Again, we're really leaning on the model's inbuilt knowledge of the GitHub API. Here we can see that these very simple descriptions here led to an extremely complex well actually not that complex implementation but like this small amount of words led to a lot of code. Very cool. Now before we get this running I actually want to instead of using standard IO I want to use prompt 3 to convert it to SSE instead. I'm on WSL and I've not been able to get cursor working with MCP servers using standard IO on WSL. is quite complicated. In the supporting information, I've got a few battle scars kind of recorded here. I've only been able to get this to work using the Express framework. I tried getting it to work with Hono, but wasn't able to. And I've added here the example implementation that I've got in an article on AI hero. The steps to complete are pretty simple here. So, let's open up a new chat with nothing in the context. Bug it in and get it going. So, it knows to install Express. Very nice. And by doing it via PNPM, it's installing the latest versions. So, as expected, it's done a really good job basically copy and pasting the code that was in the supporting information down into this express area here. It's done something slightly odd in the package.json, which is added this actual start script here. Not sure why it's done that. I didn't tell it to do that. I suppose it's useful in some sense, but uh we're going to be using the dev script here, so we don't really need this start one. In fact, I'm just going to delete it. offscreen. I've just gone away, grabbed a GitHub personal access token with the right scopes, and I've added it to my M file. So, I should now be able to run PNPM rundev, and the server is running on port 8080. In my cursor setup, I have it pointed to localhost 8080 SSE. I was getting an error here saying client closed. Okay, it turns out that deleting this line app.express.json. Going back to cursor settings, refreshing, and now everything is working. So now that cursor is hooked up to my MCP server, we can say tell me about the open PRs on map pock everite which is one of the repos I have. It's telling me to call an MCP tool list pull requests. Okay, sounds great. We can go into here and actually see what was returned from this. So we got the result inside here. Big load of data from the GitHub API. There is probably too much data here. I would probably go in and kill some of this because if we plow all of this data into our LM, we're going to run out of context window pretty quickly. But in this case, it was able to summarize it for us. So, we've got PR114 fixed chart colors by dogpaw hat. I will get to this PR, I promise. I've just got a lot on and add a failing test for the AI SDK image traces by Oh, that's me. Okay. Created by the repository owner 31st, 2024. Cool. All this to say, we've hooked up our MCP server to our MCP client and can now start doing stuff with it. Let's dive back into our codebase and start looking at a couple of refactors that I think I want to make. Yeah, this isn't vibe coding where we don't care at all about the code. We actually do care about the code because the code is the input to our LLM. And funnily enough, things like uh file structures and things, things I've never actually really cared about as a proper dev, they're kind of useful tools to be able to communicate to the LLM quickly what a file is for. If our file structure is confusing or too diffuse or or strange, then it's going to kind of trick the lm into doing weird weird things. And currently we have one enormous like main.ts file which is I think doing a bit too much. We kind of it's declaring the server transport. It's declaring the server initially. It's adding all of these tools onto here. It's just doing too much and it means that the lm might end up doing things like adding a random tools file and then not bringing it back to this. So in this prompt, we are breaking up the large files into smaller chunks. Each file may contain multiple GitHub actions or tools, but they should be split by domain and ensure that all major files are added to the important files.m MDC file. Again, just keeping that up to date. Let's start a new chat. Paste this in and off we go. Okay, our refactor is complete and we now have a new file structure. We now have source GitHub with a bunch of functions and tools underneath it. We also have a server, I suppose, with the app and the transport. And finally, we have main.ts, which basically just has an MCP server. Then says, register GitHub tools, create the Express app, and tell it to listen. The GitHub functions look pretty similar to what they had before. It's just split up into different files. So, we got issues and pull requests currently. We do have a barrel file here, which uh in libraries I don't love, but is fine for this, I suppose. But crucially, our important file stuff here has all been added. You can see how the file structure has an impact on the way that the LLM understands your codebase. If I was extending this out to a massive massive codebase, I would probably not have all of the individual issues and pulls stuff here because GitHub would know the function of the folder and the folder would essentially be the way that it could then discover all of the information within that feature. This also means you could have cursor rules attached to different folders as well. For instance, this one is autoattached where we could say, oh, let's say it only matters on source GitHub, for instance, and on all files in there. But since this one is properly global, let's just have it always attached to every conversation. So now our refactor step is complete. Let's actually add a new feature here to test out all the infrastructure that we've now built. I want to be able to check the status of actions on the MCP server. In other words, check out workflows in GitHub using my cool thing. Let's bung this in again with no context needed and just let it work. We can see by the way that the way cursor indexes stuff here, we have listed items in GitHub source functions. So, it's actually looking in the folder for the files, and it can see pretty clearly that it just needs to add a new domain into here in adds.ts into GitHub functions. Okay, it's now finished, and we can see that we've actually become the victim of our very first weird weird screw-up. First of all, the LM appears to have hallucinated the idea that there is a create tool function somewhere in the repo. So, it's completely just lost the plot and imagined this create tool.js file. then used that invented its own API here in order to create these list workflow runs tool. So I'm actually going to rerun this and edit the steps to complete. The issue here is that the LLM didn't go and look for the existing implementations to copy them. This is a context window problem. It didn't have the right stuff in the context window in order to be able to go and like find the right solution. I could take the existing implementation and put it in the supporting information, but I think it's just cleaner to tell it to go and look at the existing implementation first. So I've now told it in the start of the steps to complete, look at the existing implementations of functions and tools in the correct directory. Load them into your context window. Be very specific. So let's take this entire prompt and stick it in the chat window. We're actually going to use this chat here. We're going to go all the way up to the top and we're essentially just going to paste it in here instead. So, we're going to override the previous chat message and then just go bam. This means that all of the code that we had before essentially gets removed. So, we don't no longer have that dodgy action stuff. You can see it's all been removed. If I move my face, just go over here for a sec. All the action stuff has been removed from functions and tools. We can see now that thanks to our explicit instruction, it's going first, let's look at the existing GitHub functions and tools. Let me check the implementation of one of the existing function files. Let me check how these things are exposed to tools. It's reading the correct files, loading them into its context window. Okay, we now have an actions file that looks very much like the other actions that we had here. Again, it's relying on its training data, really spitting out these Octo functions. And here we can see the actions are much more like it. Just like this issues file here where we have register issues tools, we have an actions thing here that says register action tools. And so the previous hallucination, I mean, was that our fault? Was it the LLM's fault? Maybe the LLM should have been smart enough to understand that it should have looked in those files first. But honestly, I put this down to me not thinking ahead, not understanding what the LM's context was supposed to be. It had to have those files in context. If it didn't have the files in context, it had no idea what it was going to do. So, explicitly telling it, load those files into context and essentially copy them is the way to go. But there we go. In a series of five prompts, we have a bunch of stuff that kind of looks like an MCP server. We have a bunch of folders that will allow us to add features here where the LLM understands what it's supposed to do. We have all the infrastructure needed to run this as an MCP server locally. And we've done this with prompts that use a threesection heading kind of thing. They explain the problem that we're supposed to be solving. They give the LLM any supporting information it needs like external resources. And then you have the explicit steps to complete, which is really important. I hope you enjoyed this. I definitely enjoyed making this and I'll be back with more."
Lo2SkshWDBw,How to Vibe Code MCP Server in 10 Minutes with AI and Cursor,"In this video, we build two MCP servers using Cursor AI and Claude 3.7. MCP (Model Context Protocol) is becoming the new universal standard for AI integrations. Learning it now puts you ahead of 99% of developers and AI creators.

Useful Resources for Building MCPs:
* Antrophic's official MCP documentation: https://modelcontextprotocol.io/quickstart/server
* MCP Documentation for LLMs: https://modelcontextprotocol.io/llms-full.txt
* Obsidian-MCP: https://github.com/DataScienceDisciple/obsidian-mcp
* Obsidian Local Rest API https://github.com/coddingtonbear/obsidian-local-rest-api

🔗 Subscribe to my newsletter for more updates: aisynthesizer.beehiiv.com

Timestamps:
0:00 Intro
0:43 Why MCP Matters
1:45 Building First MCP
5:59 Building MCP with Cursor AI
8:45 How I'm Using Obsidian MCP
9:43 Outro

#mcp #claude #vibecoding",4176,https://i.ytimg.com/vi/Lo2SkshWDBw/maxresdefault.jpg,2025-03-29 01:08:49,605,Luke Skyward,https://www.youtube.com/watch?v=Lo2SkshWDBw,"There's a new game-changing protocol called MCP that's rapidly becoming the universal standard for connecting AI to data and tools. In the next 10 minutes, you'll vibe code not one but two working MCP servers, even if you're brand new to MCP or coding. First, we'll build a straightforward Weather MCP to grasp the basics. Then we'll level up and create a powerful Obsidian MCP, enabling seamless integration of AI into your Obsidian second brain. Learning MCP right now places you in the top 1% of developers, creators, and builders in the AI world. This is your chance to join them. Ready? Let's dive in. What's all this hype around MCP anyway? Imagine building web apps without APIs. Every single integration would require custom code tailored specifically for each service. It would quickly become messy, complex, and inefficient. API solved that by providing a standardized way for software to talk to each other, simplifying app development dramatically. That's exactly what MCP is doing, but specifically for AI models. Until recently, if you wanted your AI models like GPT, Claude, or Gemini to interact with tools, you could do that via function calling. However, each model provider had its own format for function calling, meaning you'd have to rewrite your integration for each model. MCP changes all of that. It introduces a single standardized protocol, meaning you write your integration once and it instantly works across every major AI model. Adopting MCP early puts you miles ahead, just like developers who adopted APIs early were years ahead of everyone else. Let's get hands-on. First, we'll build a simple weather MCP from scratch following Anthropic's official documentation, giving your AI assistant real-time weather access in the US. We'll follow the quick start for server developers. You can find a link to it in the video description. We're going to build a weather server that we can use in the cloud desktop. It's the easiest way to start using your MCP servers because Claude desktop integrates with MCP servers very well. So before we start, make sure you have Cursor and Claude Desktop installed because these will be the main tools we're going to use in the video. Before we start writing any MCP code, we need to set up our project environment. First, we'll create a new directory called weather, initialize our node project, and install the necessary packages. the model context protocol TypeScript SDK which we'll use to build the MCP server and zod for data validation. Next, we create a src folder inside our project and add an index.ts file which is where all our MCP code will live. We also need to quickly update our project configuration. We'll modify the type key in package.json to module and include a build script. To do that, we can copy and paste the JSON from MCP quickart into the cursor composer and let AI do the edit for us. Then we'll create a tsconfig.json to tell Typescript exactly how to compile our code. Now our project is ready and we're set to start building our MCP server. Inside our index.ts file, we start by importing the MCP SDK. We set up a new MCP server instance named weather, specifying version 1.00. Next, let's add the helper functions our MCP tools will use behind the scenes. We'll start by implementing a general purpose function to fetch data from the National Weather Service API. This helper handles HTTP requests. Then, we'll define a simple formatting function for weather alerts. It takes the raw alert data returned by the API and formats it into readable text that our AI assistant can easily understand and display. With these helper functions and interfaces in place, we're ready to implement our MCP tools. This is the most crucial part as this is where the actual logic is implemented. We register tools using the server.tool method, providing each tool with a clear, detailed description. This description is essential as Claude and other AI models rely on it to know exactly when and how to use each tool. Our first tool, get alerts, fetches active weather alerts for a specific US state. We clearly define its parameter, a two-letter state code, such as CA for California. The handler function fetches alert data from the National Weather Service and formats the information. The second tool, get forecast, allows AI assistance to retrieve the weather forecast for a given location. Here we specify the latitude and longitude parameters along with their acceptable ranges. Our function then uses the coordinates to retrieve forecast information and formats the response for easy readability. And that's it. By clearly defining our tools, we've given everything to start using our MCP server. Finally, we add the main function that launches our MCP server. Then we need to build our project using the mpm run build command to make the server ready. The build command creates index.js js file which will be used to connect with claude desktop. Now let's configure claude desktop to use the weather mcp server. To get the configuration file for mcps, head over to claude desktop. Go to settings developer and click edit config. Then open the config.json file in the cursor IDE. Now we need to copy and paste the server configuration from the quick start guide and change the path. We can get the absolute path to the built server by using pwd command from inside the directory with the index.js file. Once we save and restart Claude desktop, we see the new MCP tools appear under the hammer icon, confirming our server is successfully integrated. That's it. Now our MCP server should be fully operational within Claude desktop. We can test it by asking Claude for the weather forecast for LA. But what if you need something more powerful? Let me show you how to cut your MCP development time in half by letting AI do the coding for you. I'll show you the process by creating Obsidian MCP, an AI assistant that will plug into your second brain notes. We're going to speed up the development of the MCP using Cursor together with the Claude 3.7 AI assistant. Here's how to set yourself up for success. Step one, add MCP documentation to the cursor. To let Claude know how to properly build an MCP, add the official MCP documentation into cursors docs section. If you're using a specific SDK like TypeScript or Python, you can add that documentation too. Step two, define clear requirements for your MCP. Next, we create a new file named requirements.md in cursor. This file provides clear structured instructions to Claude. Think of this as your project blueprint detailing exactly what functionality your MCP server should have. You can reference code snippets to guide the server's implementation. Step three, include API documentation for tools. Most MCP servers connect with external APIs. Since we're building an Obsidian integration, we'll add documentation of the local REST API for Obsidian directly into Cursor Docs. This allows us to tag the documentation in Cursor Composer when building the MCP. With these steps completed, we're fully prepared to let AI handle most of our MCP coding. Let's jump straight into it and watch how quickly Cloud can help us build our Obsidian MCP. First, I fed our requirements MD and MCP documentation into Cursor, making sure the AI had everything it needed to generate valid MCP code. Cursor generated a complete setup tools for searching, reading, and writing files, plus a summarization prompt and main server code, but it didn't automatically work in Claude desktop. This is why I instructed cursor to review the MCP TypeScript SDK and correct the codebase, ensuring it aligned with the MCP specification. Cursor searched web for relevant TypeScript SDK documentation and updated the repository, ensuring it aligned with the MCP specification and could be recognized by Claude Desktop. Finally, I noticed my Obsidian API key for the local API plug-in wasn't passing through when launching Claude. cursor identified that the environment variable needed to be set in the cloud desktop configuration file so my server would have access at runtime. After these iterations, the Obsidian MCP now runs seamlessly inside Claude desktop. With this MCP, I can search, update, and write to my notes in Obsidian straight from Claude. Let's see how this Obsidian MCP can actually help you in a real world scenario. In my own Obsidian vault, I keep summaries of YouTube videos from my favorite creators like Greg Eisenberg in dedicated notes. Each note stores transcripts, key takeaways, and insights. The problem? When I need to recall a specific detail, I'm stuck manually reviewing a bunch of notes. With the Obsidian MCP, I can simply ask Claude to search those notes and highlight the most important insights. No more scanning through giant text files. Claude can also dig deeper into any snippet I find interesting, using that context to generate fresh ideas, expand content outlines, or even draft business plans. Essentially, the Obsidian MCP takes the hassle out of manual note review. It transforms your vault into an interactive knowledge base that Claude can query and analyze, so you get immediate on demand access to all those valuable insights you've collected. You now have everything you need to start building powerful MCP integrations right away. But why stop here? Check the video description for resources that I used when building my MCP. You can also start using the Obsidian MCP as I published it to my GitHub. Subscribe for more practical AI insights. Hit the bell so you don't miss out."
GtvDowKlgEA,Let’s Vibe Code an MCP Server in 10 Minutes,"Learn how to build with AI 🚀 Join our Skool community: https://www.skool.com/vibe-code-studio/

Learn how to vibe code your own action-taking AI assistant using an MCP Server!

In this step-by-step tutorial, we’ll use Cursor, Claude, and the Python MCP SDK to create a powerful, custom AI assistant that can manage tasks, return quotes, and connect to real-world tools — all without writing a single line of code.

This is the easiest way to go from “just chat” to “real action” with Claude AI using your own MCP server.

You’ll learn how to:
✅ Use Cursor for AI-powered, no-code development
✅ Auto-generate an MCP server using AI
✅ Add custom tools (task manager, quote bot & more)
✅ Connect your server to Claude and test in real-time

⏱️ Timestamps
0:00 – Intro
0:49 – Building our MCP Server
1:09 – What Is MCP? The USB-C for AI Tools
1:44 – Setting Up Cursor + Adding MCP Documentation
2:40 – No-Code Prompting: Auto-Generate Your MCP Server
3:30 – Explaining the Code: Imports, Tools, Decorators
4:27 – Local Testing with MCP Inspector
5:15 – Connecting Your MCP Server to Claude AI
6:05 – Live Demo: Claude Uses Hello World Tool
7:00 – Upgrade: Add a Motivational Quote Tool + Task Manager
8:43 – Walkthrough of New AI Tools (Quotes & To-Dos)
09:34 – What’s Next: More AI Assistant Features with MCP

📚 Resources & Links
https://modelcontextprotocol.io/tutorials/building-mcp-with-llms
https://github.com/modelcontextprotocol/python-sdk

More videos on MCP
https://www.youtube.com/watch?v=mCY6iwbXQqs
https://www.youtube.com/watch?v=6Gniaj2fHXI&t
https://www.youtube.com/watch?v=mrOuyuLq8qw&t

#AIassistant #MCP #nocode #ClaudeAI #CursorAI",975,https://i.ytimg.com/vi/GtvDowKlgEA/maxresdefault.jpg,2025-05-23 14:09:30,631,GritAI Studio,https://www.youtube.com/watch?v=GtvDowKlgEA,"Want to learn how to build your own MCP server without writing a single line of code yourself? This is the video for you. Wow, look at that. Cursor has completely upgraded our MCP server. Now, can you add a task for me? Add finish editing my YouTube video to my task list. Can you tell me what tasks I have on my to-do list? Now, mark that video editing task as complete. Want your AI to remember your tasks, track your to-dos, and actually help you organize your life? Or maybe you want to share your own personal knowledge base with your AI assistant. Today, I'll show you how to build the bridge that connects AI to your world. Hey everyone, welcome back to Build with Grit. I'm Alex and today I'm going to show you how to take your AI assistance and AI applications to the next level with your very own MCP server. We're going to vibe code our own MCP server using cursor and Python. And the best part, the AI will do all the heavy lifting for us. If you've been wondering what MCP is, the model context protocol is basically the USBC cable that lets AI assistants like Claude actually interact with your computer, apps, data sources, and even external tools. It's the powerup that transforms AI from being simple chat bots into a powerful tool that can actually access and use our data and applications. I've got a more detailed video about MCP on my channel if you want a deep dive. We'll cover more advanced use cases in future videos, but today we're laying the groundwork. All right, let's get started. First, we need to set up Cursor, which is an amazing code editor built for working with AI. If you're new to Cursor, we've made a whole series on getting started here. Now, here's a pro tip. To get the best results, we're going to add the MCP documentation to cursor. This helps the AI understand exactly what we're trying to build. There are several software development kits available to build MCP, but today we'll use the Python MCP SDK that takes care of all the plumbing for us, making it super easy for us to create an MCP server. Perfect. Now we're ready to build our MCP server. Let's start with the simplest version possible, a basic oneshot MCP server that just prints a message. I'm going to prompt cursor to generate this for us. It helps to be a bit specific, so feel free to steal my example. Watch how easy this is. Create a simple MCP server in Python following the instructions in these docs. Add a tool that simply returns a hello message when called. Now, I'll also add a few specifics at the end here. Add a main method that runs our server with MCP.Run. Use the stdio transport protocol. And that's it. Let's try it. Look at that cursor generated a complete MCP server for us. Let me explain what's happening here. Now, here are the imports we need. And this part sets up the MCP server using the Python fast MCP. This is our tool function decorated with the tool decorator. The Python SDK takes care of all the advanced stuff for us like generating the MCP protocol messages, translating these to the wire format, establishing connections, and everything. We don't need to worry about any of that. Now, in this simple example, we're focused on tools for our MCP server, but there are a few other concepts that we will cover in future videos, namely prompts and resources. Right now, most applications that implement the MCP client supports tools, but very few currently support prompts or resources. So, for now, let us stick with tools. Now, there are a few ways to test our server. We can use the MCP inspector, which let us test while we're developing. We can simply run MCP dev and the name of our file and then access the inspector through our browser. Let me show you. Here we can connect to the MCP server. Then we can list the available tools. And here we can run our newly created tool. See, super easy. So now, let's test this out with Claude. We'll need to install the MCP server and configure Claude to use it. We can use the MCP command line tool to install it. This will add the required configuration to our Claude config file. We can also view this config by heading over to settings, developer, and then edit config. Here you can see the config file. Now you can use this same config for instance in cursor if you want to add the MCP server to cursor. What's actually happening here is that we're instructing Claude to run this server. So we're essentially directing Claude to start this server locally. That means that we don't have to actually run the server. Claude will do it for us. Let's open up Claude and see if this is working. Great. Now, let's test it by asking Claude to use our MCP server. Claude, can you use my hello world MCP function? See, it works. We just built a functioning MCP server without writing a single line of code ourselves. Pretty cool, but it's clearly not very useful at this stage. Let's make it a bit more useful and relevant. These examples are still basic, but you should be able to build from here to whatever you want to solve with MCP. Let's upgrade our MCP server to do two things. one, read from a file containing motivational quotes and return a random quote. And two, manage a simple to-do list where we can add, view, and complete tasks. So, here's the prompt that I'll give to cursor. Enhance our MCP server to add two new functionalities. A tool that reads from a quotes.json file and returns a random motivational quote. a simple task manager that gives us a tool to add tasks, a tool to list tasks, and a tool to mark tasks as complete, storing them in a tasks.json file. Let's run it. We can of course also use AI to populate our motivational quotes database. Just prompt cursor to fill the file with motivational quotes. Wow, look at that. Cursor has completely upgraded our MCP server. Let me show you the key additions. Here's how it loads and selects random quotes. And this part here handles creating and managing the tasks file. These tools handle adding, listing, and completing tasks. Now, let's restart Claude and test this out. We've added tools, but we don't need to change the config file. Claude will always ask the server what tools it has available to use. Claude, can you give me a motivational quote using my MCP server? Now, can you add a task for me? Add finish editing my YouTube video to my task list. Can you tell me what tasks I have on my to-do list? Great. Now mark that video editing task as complete. Look at that. We just built a fully functional MCP server that can help Claude manage our tasks and provide motivational quotes. And we didn't write a single line of code ourselves. There are of course great solutions out there already for managing tasks, but the point was to demonstrate how easy it actually is to create your very own MCP server with any logic you want. This opens up so many possibilities for how you can use AI assistance in your daily life. Imagine extending this to interact with your calendar, email, or any other application. The possibilities are endless. This is just scratching the surface of what's possible with MCP. In the next video, we'll expand our MCP server to more advanced use cases like connecting to any API. So, make sure you hit that subscribe button to not miss out on future videos. If you found this video helpful, please hit that like button and subscribe for more Build with Grit videos where we explore practical ways to enhance your productivity with AI. Drop a comment below with what type of MCP functionality you'd like to see in a future video, and I'll see you in the next"
